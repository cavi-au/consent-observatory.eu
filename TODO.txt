

forbedringer af webscraber tool.

1. Improve scraper blocking workarounds: the more popular websites have ways to detect scrapers and block them,
which has stopped me from being able to collect data on them. I was hoping workarounds could be implemented.
Perhaps the Node-Unblocker library might be something easy to add? https://scrapfly.io/blog/web-scraping-with-node-unblocker/

2. URL resolution: the library currently doesn't cycle through different protocols when one of them doesn't work,
e.g., try both http, https, and www. I currently manually adjust this in the page-analyzer.js file ,
but it would be good if this could just be built in.

I think both of these adjustments would reduce the number of errors I currently get. I've attached a recent error log to show the kind of things I run into.
se zenhub fil fra midas


TODO unser /analysis/status
beskriv at filen automatisk bliver sletter om xxx dage, timer, minutter. Hvis man gerne vil oprette en ny analyse inden kan man slette filen ved at trykke på [DELETE] knap
så slettes den med det samme ...

er det et sikkerhedsmæssigt problem at browseren kan gemme url til status-side, skal den lave en session og sætte en cookie o redirecte i stedet, og så trækker den jobStatusId fra session,
så browseren ikke gemmer url?

SE MIDAS email ang forside og about side..., der er også nogle asset filer...
lav også Help side se tlf..

web-scraper update
- update alle dependencies og test, få midas til at teste
- lav _ til #
- så den default'er til https i stedet for http ved ingen protokol, og så næste strategi er http
- test nogle af den fejl midas sendte, kan de afhjælpes?
- OPDATER user-agent til en nutidig, lav så den kan sættes som option
- push til npm, og integrer i denne app
